\section{Introduction} \label{sec:intro}
\emph{Contextual bandits} \cite{Langford2007NIPS,Lu2010ICAIS:CMAB} is an important extension of the classic multi-armed bandit (MAB) problem \cite{Auer2002ML:UCB}, where  the agent can observe a set of features, referred to as \emph{context}, before making a decision. In each round, the agent chooses an action and receives a random reward with expectation depending on both the context and action.
To maximize the total reward, {the agent needs to balance between taking the best action based on the historical performance (exploitation) and exploring the potentially better alternative actions under a given context (exploration).}
This model has attracted much attention as it fits the personalized service requirement in many applications such as clinical trials \cite{Lai2012SA}, online recommendation \cite{Li2010WWW:LinUCB,Tang2013CIKM}, and online hiring in crowdsourcing \cite{Hassan2014UIC}.
Much work has been done to achieve sublinear regret in contextual bandits under different context-reward models \cite{Langford2007NIPS,Slivkins2011COLT}, and more recent work \cite{Agarwal2014ICML:CMAB} focuses on computationally efficient algorithms with minimum regret.

However, traditional contextual bandit models do not capture an important characteristic of real systems: in addition to time, there is usually a cost associated with the  resource consumed by each action and the total cost is limited by a budget in many applications. \blue{Taking crowdsourcing \cite{Hassan2014UIC} as an example, the budget constraint for a given set of tasks will limit the number of workers that an employer can hire. Another example is the clinical trials, where each treatment is usually costly and the budget of a trial is limited.
Although budget constraints have been considered in non-contextual bandits, e.g., total-budget constrained MAB \cite{Tran2010EpsFirst,Tran2012AAAI:MAB_BF,Babaioff2012EC:DP,Badanidiyuru2013FOCS} and individual-arm budget constrained MAB \cite{Jiang2013CDC,Slivkins2013TR}, the results are inapplicable in the case with observable contexts.}

In this paper, we study contextual bandit problems with budget and time constraints, referred to as \emph{constrained contextual bandits}, where  the agent is given a budget $B$ and a time horizon $T$.
In addition to a reward, a cost is incurred  whenever an action is taken under a context. The bandit process ends when the agent runs out of either budget or time.
The objective of the agent is to maximize the expected total reward subject to the budget and time constraints.

% challenges of the problem:
% root in the interactions between the information acquisition and decision making
% what interactions: 1) coupling among contexts and actions under the same context; 2) coupled evolution; 3) error propagation (more related description?)
% making a decision need to balance: obtained information, the immediate reward and the long term reward, which depends on the remaining budget and the future of the system. 
% even the oracle is challenging.

The time and budget constraints significantly complicate the design and analysis of bandit algorithms, because they introduce complex interactions between information acquisition and decision making. 
In traditional MABs without budget constraints, the oracle algorithm is trivial - should one know the statistics of all expected rewards \emph{a priori}, one can always select the best action. In constrained contextual bandits, however, the oracle has to balance the immediate reward and potential future reward.

\blue{The above constrained contextual bandit problem is a special case of Resourceful Contextual Bandits (RCB)
\cite{Badanidiyuru2014COLT}. In \cite{Badanidiyuru2014COLT}, RCB is studied under more general settings with possibly infinite contexts, random costs, and multiple budget constraints.
%A Mixture\_Elimination algorithm is proposed and shown to achieve {regret}  $O(\sqrt{T})$ regret. \wu{The regret is different in \cite{Badanidiyuru2014COLT}, where they restrict to a finite policy set and use the optimal policy in the set as the benchmark. May need to discuss this somewhere.} However, the Mixture\_Elimination algorithm is quite complex and the design of computationally efficient algorithms for such a general setting is still an open problem.
A Mixture\_Elimination algorithm is proposed and shown to achieve  $O(\sqrt{T\log T})$ regret.
However, the benchmark for the definition of regret in \cite{Badanidiyuru2014COLT} is restricted to a finite policy set, and the design of computationally efficient algorithms for such general settings is still
an open problem.}

{To tackle this problem, we consider a more restricted  model with finite discrete contexts, fixed costs, and a single budget constraint.
This simplified model is \blue{ verified}  in certain scenarios such as clinical trials \cite{Lai2012SA} and rate selection in wireless networks \cite{Combes2014Infocom}, where the contexts are quantized, the cost of each action can be {\it a priori} estimated, and only the total cost are limited.
%We also assume that the distribution of contexts is known to the agent as in \cite{Badanidiyuru2014COLT}.
\blue{More importantly,} these simplifications allow us to design easily-implementable algorithms that achieve $O(\log T)$ or $O(\sqrt{T})$ regret, which is defined more naturally as the performance gap from the \emph{oracle algorithm}, i.e., the optimal algorithm with known statistics.
%The main results obtained from these simplified settings will provide insight for the design and analysis of algorithms in more general scenarios.

Even with simplified assumptions considered in this paper, the constrained contextual bandits are still challenging
due to the budget and time constraints. The main  challenge roots in the difficulty of design and analysis of the oracle algorithm.
In fact, with known statistics of bandits, the problem becomes a finite-horizon Markov decision process (MDP), where the system state can be described by the current context, the remaining time, and the remaining budget. Theoretically, such a MDP can be solved by dynamic programming (DP). However, using DP in our scenario is challenging: first, the implementation of DP is computationally complex due to the curse of dimensionality; second, it is difficult to obtain the benchmark for regret analysis, since the DP algorithm is implemented in a recursive manner and the explicit representation for its expected total reward is hard to obtain; third, it is difficult to extend the DP algorithm to the case without known statistics, because it is difficult to evaluate the impact of estimation errors on the performance of DP-type algorithms. \blue{To address these difficulties, we start with unit-cost systems, where the costs for all actions under all contexts are identical and normalized as one. We first study algorithms in unit-cost systems and extend the results to more general systems in Sections~\ref{sec:unknown_dist} and \ref{sec:het_cost}.}

%Badanidiyuru el al. \yrcite{Badanidiyuru2014COLT} avoid the above issues by limiting the benchmark in a given algorithm set, and propose a Mixture\_Elimination algorithm that achieves  $O(\sqrt{T})$ regret for any system settings (so called instance-independent). However, it is usually expected to achieve logarithmic regret for a given system setting (instance-dependent) as many studies on MAB \cite{Lai1985AAM,Auer2002ML:UCB}. In addition, the regret of the Mixture\_Elimination algorithm is increasing as the cardinality of the algorithm set, which could be extremely large in systems with many contexts and possible actions. Moreover, a critical issue of  the Mixture\_Elimination policy is the computation complexity and no any computationally efficient implementations are available currently as discussed in \cite{Badanidiyuru2014COLT}.

Our first contribution is a near-optimal approximation of the oracle algorithm.
We resort to approximations by relaxing the hard budget constraint to an average budget constraint.
Specifically, we first obtain an upper bound on the expected total reward by solving a linear programming (LP) problem that maximizes the expected reward with average budget constraint $B/T$.
This upper bound provides a benchmark for the regret analysis later. However,
the policy from this static LP problem is suboptimal in practice as it  does not take the remaining time $\tau$ and remaining budget $b_{\tau}$ into account. Hence, we propose an Adaptive Linear Programming (ALP) algorithm that replaces the average budget constraint $B/T$ by the \emph{average remaining budget}, i.e., $b_{\tau}/\tau$.
Somewhat surprisingly, with this simple adaptation for the budget constraint, the ALP algorithm achieves very good performance. We show that ALP achieves an expected total reward within a constant (independent of $T$) from the optimum, i.e., $O(1)$ regret, except for certain boundary cases. This $O(1)$ regret implies that the regret due to the linear relaxation could be neglectable with appropriate design  in the unknown expected reward case.

We then study algorithms for the case where the expected rewards are unknown (but the context distribution is known as in \cite{Badanidiyuru2014COLT}).
We note that the ALP algorithm only requires the ordering of the expected rewards rather than their actual values.
This property allows us to combine  ALP with estimation methods that can correctly rank the expected rewards with high probability in a short period. In this paper, we combine with
the upper-confidence-bound (UCB) algorithm \cite{Auer2002ML:UCB} and propose a UCB-ALP algorithm.
We show that  the UCB-ALP algorithm achieves $O(\log T)$ regret except for certain boundary cases, where its regret is $O(\sqrt{T})$.
For the non-boundary case, the UCB-ALP algorithm is order-optimal as its regret matches the lower bound $O(\log T)$, which can be obtained by the techniques in \cite{Lai1985AAM}. We note that a recent work \cite{Agrawal2014EC} proposes UCB-type algorithms  for linear contextual bandits with budget constraint. However, \cite{Agrawal2014EC} focuses on static contexts where the contexts are bounded to actions. In principle, one can extend the results to the case with randomly arrival contexts by treating each ``context-action'' mapping as an action, but the action space then becomes exponentially large as discussed in the supplementary material. Moreover, the algorithms in \cite{Agrawal2014EC} use fixed budget constraints and achieve $O(\sqrt{T})$ regret, while our proposed UCB-ALP algorithm achieves $O(\log T)$ regret by using an adaptive budget constraint.


Finally, we extend the proposed algorithms to more general systems with unknown context distribution or heterogeneous costs. For the case with unknown context distribution, we propose to use the empirical estimates of the context probabilities instead of their actual values. We show that such Empirical ALP (EALP) and UCB-EALP algorithms achieve similar performance as ALP and UCB-ALP, respectively. For the case with heterogeneous costs, we generalize the ALP algorithm when the statistics are known and  propose an $\epsilon$-First ALP when the statistics are unknown, which is also shown to achieve the logarithmic regret except for the boundary cases.
%In this case, the agent needs to consider all actions even with known statistics, and a coupling effect occurs among actions under a context. {However}, the ALP algorithm can be extended to heterogeneous-cost systems with known statistics and is shown to achieve similar performance as in unit-cost systems. In the case without knowledge of expected rewards, the UCB-ALP algorithm is still applicable when the costs of all actions are identical under the same context.
%We believe the main results of this paper also provide insight for the design of algorithms with lower regret and  for more general constrained contextual bandits, such as systems with multi-dimensional budget constraints.

\blue{In summary, we study easily-implementable algorithms for contextual bandits with budget and time constraints under discrete contexts, and show these simple algorithms works very well. Our work shows how to effectively combines information learning and budget allocation to achieve logarithmic or sublinear regret in constrained contextual bandits. Moreover, although the intuition behind the proposed algorithms are natural, the analysis of these algorithms is non-trivial and is an important part of our contributions.
In particular, the methods for analyzing the evolution of system status under adaptive algorithms and bounding estimation errors provide insights for more general resource constrained problems without \emph{a priori} knowledge.}




%In summary, the main contributions of this paper are:
%\begin{itemize}
%\item{Starting with a unit-cost assumption, we first study algorithms in systems with known statistics. We propose a near optimal ALP algorithm to approximate the oracle solution.}
%\item{Combining the insight from the known-statistics systems with the UCB method, we propose a computational-efficient algorithm, UCB-ALP, for systems without prior knowledge of the expected rewards. We show that UCB-ALP achieves $O(\log T)$ regret in general multi-context systems except in certain boundary cases (where it achieves $O(\sqrt{T})$ regret).}
%\item{We extend the ALP and UCB-ALP algorithms to heterogenous-cost systems. We show that ALP  achieves similar performance as in unit-cost systems and UCB-ALP can be applied when all actions have identical costs under the same context.}
%\end{itemize}

