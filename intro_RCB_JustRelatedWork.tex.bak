\section{Introduction} \label{sec:intro}
The contextual bandit problem \cite{Langford2007NIPS,Lu2010ICAIS:CMAB} is an important extension of the classic multi-armed bandit (MAB) problem \cite{Auer2002ML:UCB}, where  the agent can observe a set of features, referred to as \emph{context}, before making a decision. \wu{OR, Contextual bandits enhance the classic multi-armed bandit (MAB) problem with side-information, where ...} In each round, a context arrives randomly, and the agent chooses an action and receives a random reward with expectation depending on both the context and action.
To maximize the total reward, {the agent needs to balance between taking the best action based on the historical performance (exploitation) and exploring the potentially better alternative actions under a given context (exploration).}
This model has attracted much attention as it fits the personalized service requirement in many applications such as clinical trials \cite{Lai2012SA}, online recommendation \cite{Li2010WWW:LinUCB,Tang2013CIKM}, and online hiring in crowdsourcing \cite{Hassan2014UIC}.
Much work has been done to achieve sublinear regret in contextual bandits under different context-reward models \cite{Langford2007NIPS,Slivkins2011COLT}, and more recent work \cite{Agarwal2014ICML:CMAB} focuses on computationally efficient algorithms with minimum regret.

However, traditional contextual bandit models do not capture an important characteristic of real systems: in addition to time, there is usually a cost associated with the  resource consumed by each action and the total cost is limited by a budget in many applications. \blue{Taking crowdsourcing \cite{Hassan2014UIC} as an example, the budget constraint for a given set of tasks will limit the number of workers that an employer can hire. Another example is the clinical trials, where each treatment is usually costly and the budget of a trial is limited.
Although budget constraints have been considered in non-contextual bandits, e.g., total-budget constrained MAB \cite{Tran2010EpsFirst,Tran2012AAAI:MAB_BF,Babaioff2012EC:DP,Badanidiyuru2013FOCS} and individual-arm budget constrained MAB \cite{Jiang2013CDC,Slivkins2013TR}, the results are inapplicable in the case with observable contexts.}

In this paper, we study contextual bandit problems with budget and time constraints, referred to as \emph{constrained contextual bandits}, where  the agent is given a budget $B$ and a time horizon $T$.
In addition to a reward, a cost is incurred  whenever an action is taken under a context. The bandit process ends when the agent runs out of either budget or time.
The objective of the agent is to maximize the expected total reward subject to the budget and time constraints.

% challenges of the problem:
% root in the interactions between the information acquisition and decision making
% what interactions: 1) coupling among contexts and actions under the same context; 2) coupled evolution; 3) error propagation
% May not focus on these interactions, because we may need to highlight the challenges compared with classic contextual bandits
% and generally, these interactions occur in classic contextual bandits, too (the reason is that time is also one dimensionality of resource and the time horizon is a budget constraint; the first point seems not to be the interaction between the information acquisition and decision making.

% making a decision need to balance: obtained information, the immediate reward and the long term reward, which depends on the remaining budget and the future of the system.
% even the oracle is challenging.

% Challenges
\blue{The time and budget constraints significantly complicate the exploration and exploitation tradeoff, because they introduce complex coupling among contexts over time.
%In traditional MABs without budget constraints, the oracle algorithm is trivial - should one know the statistics of all expected rewards \emph{a priori}, one can always select the best action. In constrained contextual bandits, however, the oracle has to balance the immediate reward and potential future reward.
The key  challenge comes from the difficulty of design and analysis of the oracle algorithm.
%In fact, with known statistics of bandits, the problem becomes a finite-horizon Markov decision process (MDP), where the system state can be described by the current context, the remaining time, and the remaining budget.
With budget and time constraints, the oracle algorithm cannot simply take the action that maximizes the immediate reward. In contrast, it need to balance the immediate and long-term rewards based on the current context and the remaining budget.
Theoretically, dynamic programming (DP) can be used to obtain the oracle algorithm. However, using DP in our scenario incurs difficulties in both algorithm design and analysis: first, the implementation of DP is computationally complex due to the curse of dimensionality; second, it is difficult to obtain the benchmark for regret analysis, since the DP algorithm is implemented in a recursive manner and the explicit representation for its expected total reward is hard to obtain; third, it is difficult to extend the DP algorithm to the case without known statistics, because it is difficult to evaluate the impact of estimation errors on the performance of DP-type algorithms.}

\blue{To address these challenges, we consider a slightly restricted yet practical model with finite discrete contexts and fixed costs.
This simplified model is \blue{ verified}  in certain scenarios such as clinical trials \cite{Lai2012SA} and rate selection in wireless networks \cite{Combes2014Infocom}, where the contexts are quantized and the cost of each action can be {\it a priori} estimated. Starting with the unit-cost systems, we first study approximations to the oracle algorithm. With the highly desirable properties of the approximate oracle solution, we design algorithms under the bandit settings where the statistics are unknown. 
The problem studied in this paper can be viewed as a special case of Resourceful Contextual Bandits (RCB)
\cite{Badanidiyuru2014COLT}. In \cite{Badanidiyuru2014COLT}, RCB is studied under more general settings with possibly infinite contexts, random costs, and multiple budget constraints. A Mixture\_Elimination algorithm is proposed and shown to achieve  $O(\sqrt{T})$ regret.
However, the benchmark for the definition of regret in \cite{Badanidiyuru2014COLT} is restricted to a finite policy set, and the design of computationally efficient algorithms for such general settings is still
an open problem.
In contrast, our study provides insight of designing easily-implementable algorithms that achieve $O(\log T)$ or $O(\sqrt{T})$ regret, which is defined more naturally as the performance gap from the \emph{oracle algorithm}, i.e., the optimal algorithm with known statistics.}


Our first contribution is a near-optimal approximation of the oracle algorithm. \blue{This approximation makes an important step towards achieving logarithmic or sublinear regret as it reveals essential characteristics of the oracle.} Specifically, we first obtain an upper bound on the expected total reward by solving a linear programming (LP) problem that maximizes the expected reward with average budget constraint $B/T$.
This upper bound provides a good benchmark in the regret analysis later. However,
the policy \blue{derived} from this static LP problem is suboptimal in practice as it  does not take the remaining time $\tau$ and remaining budget $b_{\tau}$ into account. Hence, we propose an Adaptive Linear Programming (ALP) algorithm that replaces the average budget constraint $B/T$ by the \emph{average remaining budget}, i.e., $b_{\tau}/\tau$.
Somewhat surprisingly, with such a natural adaptation, the ALP algorithm achieves very good performance. We show that ALP achieves an expected total reward within a constant (independent of $T$) from the optimum, i.e., $O(1)$ regret, except for certain boundary cases. This $O(1)$ regret implies that the regret due to the linear relaxation could be neglectable and low regret in the unknown expected reward case is achievable by appropriately combining ALP with information acquisition mechanism.

We then study algorithms for the case where the expected rewards are unknown (but the context distribution is known as in \cite{Badanidiyuru2014COLT}).
We note that the ALP algorithm only requires the ordering of the expected rewards rather than their actual values.
This property allows us to combine  ALP with estimation methods that can correctly rank the expected rewards with high probability in a short period. In this paper, we combine ALP with
the upper-confidence-bound (UCB) algorithm \cite{Auer2002ML:UCB} and propose a UCB-ALP algorithm.
We show that  the UCB-ALP algorithm achieves $O(\log T)$ regret except for certain boundary cases, where its regret is $O(\sqrt{T})$.
For the non-boundary case, the UCB-ALP algorithm is order-optimal as its regret matches the lower bound $O(\log T)$, which can be obtained by the techniques in \cite{Lai1985AAM}. We note that a recent work \cite{Agrawal2014EC} proposes UCB-type algorithms  for linear contextual bandits with budget constraint. However, \cite{Agrawal2014EC} focuses on static contexts and achieves $O{\sqrt{T}}$ regret since it uses fixed budget constraint in each round, while our proposed UCB-ALP algorithm achieves $O(\log T)$ regret by using an adaptive budget constraint.


Finally, we study algorithms for more general systems with unknown context distribution or heterogeneous costs. For the case with unknown context distribution, we propose to use the empirical estimate of the context probabilities instead of their actual values. We show that such Empirical ALP (EALP) and UCB-EALP algorithms achieve similar performance as ALP and UCB-ALP, respectively. For the case with heterogeneous costs, we generalize the ALP algorithm when the statistics are known and  propose an $\epsilon$-First ALP when the statistics are unknown, which is also shown to achieve the logarithmic regret except for the boundary cases. The analyses for these cases are much more challenging due to the dependency (unknown context distribution) or coupling among actions (heterogeneous cost), as shown in the supplementary material.
%In this case, the agent needs to consider all actions even with known statistics, and a coupling effect occurs among actions under a context. {However}, the ALP algorithm can be extended to heterogeneous-cost systems with known statistics and is shown to achieve similar performance as in unit-cost systems. In the case without knowledge of expected rewards, the UCB-ALP algorithm is still applicable when the costs of all actions are identical under the same context.
%We believe the main results of this paper also provide insight for the design of algorithms with lower regret and  for more general constrained contextual bandits, such as systems with multi-dimensional budget constraints.

\blue{In summary, we study easily-implementable algorithms for contextual bandits with budget and time constraints, and show these simple algorithms works very well. To the best of our knowledge, this is the first work that shows how to effectively integrate  information acquisition  and decision making to achieve logarithmic regret in constrained contextual bandits. Moreover, although the intuition behind the proposed algorithms are natural, the analysis of these algorithms is non-trivial and is an important part of our contributions.
In particular, the methods for analyzing the evolution of system status under adaptive algorithms and bounding estimation errors provide insights for more general resource constrained problems without \emph{a priori} knowledge.}

