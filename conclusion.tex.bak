\section{Conclusion} \label{sec:conclusion}
%In this paper, we study computationally-efficient algorithms that achieve logarithmic or sublinear regret for constrained contextual bandits. We first focus on unit-cost systems with known statistics. We show that when the system statistics are known,  a simple ALP algorithm can achieve near-optimal performance, while tolerating certain estimation errors of system parameters.
%Utilizing the desirable properties of ALP, we propose a UCB-ALP algorithm for systems  with unknown expected rewards.
%We show that UCB-ALP achieves $O(\log T)$ regret except for certain boundary cases, where it achieves $O(\sqrt{T})$ regret.
%The ALP and UCB-ALP algorithms can also be generalized to  constrained contextual  bandits with unknown context distribution and heterogeneous costs.
In this paper, we study computationally-efficient algorithms that achieve logarithmic or sublinear regret for constrained contextual bandits. Under simplified yet practical assumptions, we show that the close interactions between the information acquisition and decision making in constrained contextual bandits can be decoupled by adaptive linear relaxation. When the system statistics are known, the ALP approximation achieves near-optimal performance, while tolerating certain estimation errors of system parameters. When the expected rewards are unknown, the proposed UCB-ALP algorithm leverages the advantages of ALP and UCB, and achieves $O(\log T)$ regret except for certain boundary cases, where it achieves $O(\sqrt{T})$ regret. Our study provides an efficient approach of  dealing with the challenges introduced by budget constraints and could potentially be extended to more general constrained contextual bandits. 